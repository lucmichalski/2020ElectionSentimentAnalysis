{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "SjTML4IQQdOV",
    "outputId": "ace99e88-e239-4182-c2dd-cdb9cc934f0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dominic/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import os\n",
    "# !pip3 install nest_asyncio\n",
    "# !pip3 install twint\n",
    "# !pip3 install nltk\n",
    "import nest_asyncio\n",
    "import string\n",
    "nest_asyncio.apply()\n",
    "import twint\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import textblob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer, WhitespaceTokenizer\n",
    "import preprocessor as p\n",
    "from textblob import TextBlob\n",
    "# nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OBVMuwlG7ZDK"
   },
   "outputs": [],
   "source": [
    "candidates = [\"michael bennet\", \"joseph biden\",\n",
    "              \"michael bloomberg\", \"cory booker\",\n",
    "              \"steve bullock\", \"pete buttigieg\",\n",
    "              \"julian castro\", \"john delaney\",\n",
    "              \"tulsi gabbard\", \"kamala harris\",\n",
    "              \"amy klobuchar\", \"deval patrick\",\n",
    "              \"bernie sanders\", \"joe sestak\",\n",
    "              \"tom steyer\", \"elizabeth warren\",\n",
    "              \"marianne williamson\", \"andrew yang\",\n",
    "              \"donald trump\", \"joe walsh\"]\n",
    "\n",
    "\n",
    "class candidate:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.tweets = pd.DataFrame()\n",
    "    def tweet_crawler(self, count=10):\n",
    "        api = TwitterClient()\n",
    "        self.tweets = api.get_tweets(self.name, count)\n",
    "\n",
    "\n",
    "\n",
    "# class Preprocess:\n",
    "#     def iterate(self):\n",
    "#         for preprocess_method in [self.remove_urls,\n",
    "#                                   self.remove_usernames,\n",
    "#                                   self.remove_numbers,\n",
    "#                                   self.remove_special_chars,\n",
    "#                                   self.remove_non_ascii,\n",
    "#                                   self.remove_stopwords]:\n",
    "#             yield preprocess_method\n",
    "\n",
    "#     @staticmethod\n",
    "#     def remove_by_regex(tweets, regexp):\n",
    "#         # print(tweets)\n",
    "#         print(tweets.loc[:, \"tweet\"])\n",
    "#         if tweets is not None:\n",
    "#             tweets.loc[:, \"tweet\"].sub(regexp, \"\", tweets.loc[:, \"tweet\"])\n",
    "#         # print(tweets)\n",
    "#         # tweets = regexp.sub('', tweets)\n",
    "#         # tweets.replace(\"\", regexp, inplace=True)\n",
    "\n",
    "#     def remove_urls(self, tweets):\n",
    "#         return Preprocess.remove_by_regex(tweets, re.compile(r\"http.?://[^\\s]+[\\s]?\"))\n",
    "\n",
    "#     def remove_usernames(self, tweets):\n",
    "#         return Preprocess.remove_by_regex(tweets, re.compile(r\"@[^\\s]+[\\s]?\"))\n",
    "\n",
    "#     def remove_numbers(self, tweets):\n",
    "#         return Preprocess.remove_by_regex(tweets, re.compile(r\"\\s?[0-9]+\\.?[0-9]*\"))\n",
    "\n",
    "#     def remove_special_chars(self, tweets):  # it unrolls the hashtags to normal words\n",
    "#         for remove in map(lambda r: regex.compile(re.escape(r)), [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\",\n",
    "#                                                                      \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "#                                                                      \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\",\n",
    "#                                                                      \"!\", \"?\", \".\", \"'\",\n",
    "#                                                                      \"--\", \"---\", \"#\"]):\n",
    "#             tweets.loc[:, \"tweet\"].replace(remove, \"\", inplace=True)\n",
    "#         return tweets\n",
    "\n",
    "#     def remove_non_ascii(self, tweets):\n",
    "#         return Preprocess.remove_by_regex(tweets, regex.compile(r\"[^\\x00-\\x7F]+\"))\n",
    "\n",
    "#     def remove_stopwords(self, tweets):\n",
    "#         stop_words = set(stopwords.words('english'))\n",
    "#         word_tokens = word_tokenize(tweet)\n",
    "#         processed_tweet = [word for word in word_tokens if not word in stop_words]\n",
    "#         return ' '.join(processed_tweet)\n",
    "\n",
    "\n",
    "class TwitterClient:\n",
    "    '''\n",
    "    Twitter Twint intialization\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        #Intializes the Twint object\n",
    "        self.twint_Api = twint.Config()\n",
    "\n",
    "    def clean_tweet(self, tweet):\n",
    "        # tweet = p.clean(tweet['text'])\n",
    "        # Check characters to see if they are in punctuation\n",
    "        nopunc = [char for char in tweet if char not in string.punctuation]\n",
    "        # Join the characters again to form the string.\n",
    "        nopunc = ''.join(nopunc)\n",
    "        # convert text to lower-case\n",
    "        nopunc = nopunc.lower()\n",
    "        # remove URLs\n",
    "        nopunc = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', nopunc)\n",
    "        nopunc = re.sub(r'http\\S+', '', nopunc)\n",
    "        # remove usernames\n",
    "        nopunc = re.sub('@[^\\s]+', '', nopunc)\n",
    "        # remove the # in #hashtag\n",
    "        nopunc = re.sub(r'#([^\\s]+)', r'\\1', nopunc)\n",
    "        # remove repeated characters\n",
    "        nopunc = word_tokenize(nopunc)\n",
    "        # remove stopwords from final word list\n",
    "        words = [word for word in nopunc if word not in stopwords.words('english')]\n",
    "        return \" \".join(words)\n",
    "    \n",
    "    def preprocess(self, data ):\n",
    "        cleaned_tweets = {}\n",
    "        newdata = []\n",
    "\n",
    "        stop_words = stopwords.words('english')\n",
    "        words = set(nltk.corpus.words.words())\n",
    "\n",
    "        for token in data.tweet: \n",
    "            newdoc = \"\"\n",
    "            #print(token)\n",
    "            seperator = \" \" \n",
    "            token = re.sub('[^A-Za-z0-9]+', ' ', token) #remove special characters\n",
    "            token = re.sub(r'\\d+', '', token) #remove numbers\n",
    "            #token = re.sub(r'\\b\\w{1,2}\\b', '', token) #remove words with <= 2 characters\n",
    "            token = \" \".join(w for w in nltk.wordpunct_tokenize(token) \\\n",
    "                    # if w.lower() in words or \n",
    "                     or not w.isalpha())\n",
    "            whitespace_token = WhitespaceTokenizer().tokenize( token )\n",
    "            #wo_stopwords_token = [x for x in whitespace_token\n",
    "            #                      if not x in stop_words]\n",
    "            newdoc = seperator.join( (whitespace_token) ).lower()\n",
    "            #print(newdoc)\n",
    "            newdata.append(newdoc)\n",
    "        \n",
    "        print(newdata)\n",
    "        cleaned_tweets['tweet'] = newdata\n",
    "    \n",
    "        newtweets = pd.DataFrame(cleaned_tweets)\n",
    "\n",
    "        return newtweets\n",
    "\n",
    "    def get_tweet_sentiment(self, tweet):\n",
    "        analysis = TextBlob(self.clean_tweet(tweet))\n",
    "        return analysis.sentiment.polarity, analysis.sentiment.subjectivity\n",
    "\n",
    "    def twint_to_pandas(self, col):\n",
    "        return twint.output.panda.Tweets_df[col]\n",
    "\n",
    "    def get_tweets(self, search, count):\n",
    "\n",
    "        #Creates a search string from the list of presidential candidates\n",
    "        self.twint_Api.Search = search\n",
    "        self.twint_Api.Limit = count\n",
    "        self.twint_Api.Format = \"Username: {username} |  Tweet: {tweet}  |  Location: {location}\"\n",
    "        self.twint_Api.Lang = \"en\"\n",
    "        self.twint_Api.Pandas = True\n",
    "        self.Output = \"output.csv\"\n",
    "        # self.twint_Api.Pandas_au = True\n",
    "        self.twint_Api.Location = True\n",
    "        self.twint_Api.Hide_output = True\n",
    "\n",
    "        twint.run.Search(self.twint_Api)\n",
    "\n",
    "        tweets_df = self.twint_to_pandas([\"date\", \"username\", \"tweet\", \"hashtags\"])\n",
    "        #cleaned_tweets = preprocess( tweets_df )\n",
    "        \n",
    "        #print(tweets_df.keys())\n",
    "        tweet_sentiment = []\n",
    "        tweet_subjectivity = []\n",
    "        \n",
    "        tweets_df = self.preprocess(tweets_df)\n",
    "        print(tweets_df)\n",
    "        # self.clean_tweet(tweets_df)\n",
    "        for tweet in tweets_df.tweet:\n",
    "            #print(tweet)\n",
    "            sentiment = self.get_tweet_sentiment(tweet)\n",
    "            tweet_sentiment.append(sentiment[0])\n",
    "            tweet_subjectivity.append(sentiment[1])\n",
    "            #print(tweet)\n",
    "            \n",
    "        print(tweet_sentiment)\n",
    "        tweets_df['Sentiment'] = tweet_sentiment\n",
    "        return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "afQ3-uIWQ4WX",
    "outputId": "cf5da96b-7daa-4b51-cd5e-8d4a90828259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not that i am aware of your point', 'mikebloomberg another wannabe president that is willing to bend over for the money goawaybloomberg pic twitter com exxfgtxovx', 'bloomberg is authoritarian af if it s him vs trump i m sitting it out and going back to not giving a shit we ll deserve what we get at that point', 'hadn t even thought of that but it is their money no matter how foolishly spent', 'much as i find many of your statements annoying this one is accurate well not the race and gender parts zero proof of that here but money absolutely bloomberg s in our system directly lets him continue while she can t', 'those of you that are supporting mikebloomberg which side of the political isle do you typically lean bloomberg election', 'hey mike can i have mile of your mile high stack of bills', 'go away', 'you realize that he will never get the black hispanic vote right', 'new show trump fakes orgasm at rally michael bloomberg polling at mark zuckerberg s dinner with trump and much more https davidpakman com pic twitter com ycldsk', 'he s wasting his time i regard his candidacy like i do gum on my shoe an irritant which needs to be forcibly removed she was quality he is fluff', 'your statement is insufficient serves primarily to remind voters that you ve just bought your way into the race while senkamalaharris has inadequate funding to continue her campaign', 'eat my ass', 'salt in wounds ass hole', 'wtf you re still wow', 'hey mikebloomberg tomsteyer instead of spending a b dollars running the same exact ad over over in every commercial break maybe try using some of that money to make a variation of commercials that run every nd rd comm break why are we seeing the same commercial repeat', 'that s not what i said at all although the reality is that trump is the biggest racist around', 'what if they all sucked because they do', 'honestly what kind of crap is this', 'mike obi wan please buy fox you are the only one who can then turn them into a true fair and balanced news station the blue wave needs your money not sure the presidency is the way to go']\n",
      "                                                tweet\n",
      "0                   not that i am aware of your point\n",
      "1   mikebloomberg another wannabe president that i...\n",
      "2   bloomberg is authoritarian af if it s him vs t...\n",
      "3   hadn t even thought of that but it is their mo...\n",
      "4   much as i find many of your statements annoyin...\n",
      "5   those of you that are supporting mikebloomberg...\n",
      "6   hey mike can i have mile of your mile high sta...\n",
      "7                                             go away\n",
      "8   you realize that he will never get the black h...\n",
      "9   new show trump fakes orgasm at rally michael b...\n",
      "10  he s wasting his time i regard his candidacy l...\n",
      "11  your statement is insufficient serves primaril...\n",
      "12                                         eat my ass\n",
      "13                            salt in wounds ass hole\n",
      "14                               wtf you re still wow\n",
      "15  hey mikebloomberg tomsteyer instead of spendin...\n",
      "16  that s not what i said at all although the rea...\n",
      "17            what if they all sucked because they do\n",
      "18                 honestly what kind of crap is this\n",
      "19  mike obi wan please buy fox you are the only o...\n",
      "[0.25, 0.25, -0.1, -0.1, 0.09999999999999999, 0.02777777777777778, 0.16, 0.0, 0.05952380952380952, 0.16818181818181818, 0.2, 0.4, 0.0, 0.0, -0.2, 0.08333333333333333, 0.0, 0.0, -0.10000000000000003, 0.26999999999999996]\n"
     ]
    }
   ],
   "source": [
    "test = candidate(candidates[2])\n",
    "test.tweet_crawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXU2hNdg-RMo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweet', 'Sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(test.tweets.keys())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "FinalProject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
